{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from src.reader import read_lusa, read_timebank\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial']\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "ROOT = Path().resolve().parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'llama2-7b',\n",
    "    'llama2-7b-chat',\n",
    "    'llama2-13b',\n",
    "    'llama2-13b-chat',\n",
    "    'llama2-70b',\n",
    "    'llama2-70b-chat',\n",
    "    'gpt3',\n",
    "    'chatgpt',\n",
    "    'gpt4',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = ROOT / \"results\" / \"prompt_selection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt = pd.read_csv(results_path / \"portuguese\"/ \"results.csv\")\n",
    "df_pt[\"language\"] = \"Portuguese\"\n",
    "\n",
    "df_en = pd.read_csv(results_path / \"english\"/ \"results.csv\")\n",
    "df_en[\"language\"] = \"English\"\n",
    "\n",
    "df = pd.concat([df_pt, df_en])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df[\"model\"] = pd.Categorical(df.model, ordered=True, categories=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = df.language.unique().tolist()\n",
    "entities = df.entity.unique().tolist()\n",
    "templates = [\"ext\", \"cls\",\t\"ext_def\", \"ext_exp\", \"cls_def\", \"ext_def_exp\",  \"cls_exp\", \"cls_def_exp\"]\n",
    "labels = [\"_ _ _\", \"C _ _\", \"_ D _\", \"_ _ E\", \"C D _\", \"_ D E\",  \"C _ E\", \"C D E\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>template</th>\n",
       "      <th>ext</th>\n",
       "      <th>cls</th>\n",
       "      <th>ext_def</th>\n",
       "      <th>ext_exp</th>\n",
       "      <th>cls_def</th>\n",
       "      <th>ext_def_exp</th>\n",
       "      <th>cls_exp</th>\n",
       "      <th>cls_def_exp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th>entity</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"27\" valign=\"top\">English</th>\n",
       "      <th rowspan=\"9\" valign=\"top\">event triggers</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.40</td>\n",
       "      <td>11.06</td>\n",
       "      <td>7.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>6.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.90</td>\n",
       "      <td>28.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.15</td>\n",
       "      <td>28.30</td>\n",
       "      <td>29.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.91</td>\n",
       "      <td>2.93</td>\n",
       "      <td>8.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.53</td>\n",
       "      <td>6.28</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>2.28</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.43</td>\n",
       "      <td>20.90</td>\n",
       "      <td>0.30</td>\n",
       "      <td>16.79</td>\n",
       "      <td>18.68</td>\n",
       "      <td>18.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>8.34</td>\n",
       "      <td>3.31</td>\n",
       "      <td>14.96</td>\n",
       "      <td>26.44</td>\n",
       "      <td>3.10</td>\n",
       "      <td>25.45</td>\n",
       "      <td>13.77</td>\n",
       "      <td>26.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>1.17</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.29</td>\n",
       "      <td>41.22</td>\n",
       "      <td>4.95</td>\n",
       "      <td>42.13</td>\n",
       "      <td>18.12</td>\n",
       "      <td>18.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.04</td>\n",
       "      <td>41.12</td>\n",
       "      <td>4.85</td>\n",
       "      <td>42.12</td>\n",
       "      <td>24.59</td>\n",
       "      <td>38.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>8.29</td>\n",
       "      <td>33.93</td>\n",
       "      <td>35.96</td>\n",
       "      <td>55.08</td>\n",
       "      <td>40.60</td>\n",
       "      <td>56.32</td>\n",
       "      <td>57.30</td>\n",
       "      <td>59.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>20.45</td>\n",
       "      <td>57.82</td>\n",
       "      <td>21.36</td>\n",
       "      <td>72.81</td>\n",
       "      <td>33.33</td>\n",
       "      <td>72.93</td>\n",
       "      <td>72.60</td>\n",
       "      <td>74.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">participants</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">time expressions</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>7.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.84</td>\n",
       "      <td>10.63</td>\n",
       "      <td>3.24</td>\n",
       "      <td>8.79</td>\n",
       "      <td>8.51</td>\n",
       "      <td>8.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>1.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>21.09</td>\n",
       "      <td>8.99</td>\n",
       "      <td>22.22</td>\n",
       "      <td>16.62</td>\n",
       "      <td>9.37</td>\n",
       "      <td>13.75</td>\n",
       "      <td>12.11</td>\n",
       "      <td>16.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>12.26</td>\n",
       "      <td>2.91</td>\n",
       "      <td>15.91</td>\n",
       "      <td>21.93</td>\n",
       "      <td>4.56</td>\n",
       "      <td>21.49</td>\n",
       "      <td>11.31</td>\n",
       "      <td>12.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>32.56</td>\n",
       "      <td>16.04</td>\n",
       "      <td>31.16</td>\n",
       "      <td>25.77</td>\n",
       "      <td>19.73</td>\n",
       "      <td>20.27</td>\n",
       "      <td>16.94</td>\n",
       "      <td>18.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>37.61</td>\n",
       "      <td>27.01</td>\n",
       "      <td>39.17</td>\n",
       "      <td>37.84</td>\n",
       "      <td>21.57</td>\n",
       "      <td>40.30</td>\n",
       "      <td>31.61</td>\n",
       "      <td>28.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>35.82</td>\n",
       "      <td>26.38</td>\n",
       "      <td>30.61</td>\n",
       "      <td>42.64</td>\n",
       "      <td>29.59</td>\n",
       "      <td>47.57</td>\n",
       "      <td>36.91</td>\n",
       "      <td>38.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>58.45</td>\n",
       "      <td>64.19</td>\n",
       "      <td>53.81</td>\n",
       "      <td>64.22</td>\n",
       "      <td>65.52</td>\n",
       "      <td>60.63</td>\n",
       "      <td>64.52</td>\n",
       "      <td>61.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"27\" valign=\"top\">Portuguese</th>\n",
       "      <th rowspan=\"9\" valign=\"top\">event triggers</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.30</td>\n",
       "      <td>29.64</td>\n",
       "      <td>3.09</td>\n",
       "      <td>27.68</td>\n",
       "      <td>19.32</td>\n",
       "      <td>19.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>3.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.27</td>\n",
       "      <td>24.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.88</td>\n",
       "      <td>27.03</td>\n",
       "      <td>23.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.57</td>\n",
       "      <td>17.56</td>\n",
       "      <td>33.37</td>\n",
       "      <td>3.19</td>\n",
       "      <td>34.33</td>\n",
       "      <td>13.50</td>\n",
       "      <td>17.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>9.12</td>\n",
       "      <td>4.78</td>\n",
       "      <td>13.27</td>\n",
       "      <td>18.21</td>\n",
       "      <td>6.08</td>\n",
       "      <td>13.76</td>\n",
       "      <td>27.24</td>\n",
       "      <td>28.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>3.76</td>\n",
       "      <td>0.78</td>\n",
       "      <td>7.73</td>\n",
       "      <td>31.75</td>\n",
       "      <td>9.86</td>\n",
       "      <td>31.31</td>\n",
       "      <td>30.53</td>\n",
       "      <td>29.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>2.60</td>\n",
       "      <td>10.84</td>\n",
       "      <td>1.57</td>\n",
       "      <td>30.04</td>\n",
       "      <td>9.31</td>\n",
       "      <td>32.14</td>\n",
       "      <td>30.85</td>\n",
       "      <td>33.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>2.26</td>\n",
       "      <td>22.42</td>\n",
       "      <td>2.80</td>\n",
       "      <td>50.13</td>\n",
       "      <td>27.42</td>\n",
       "      <td>48.91</td>\n",
       "      <td>49.67</td>\n",
       "      <td>49.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>33.16</td>\n",
       "      <td>37.91</td>\n",
       "      <td>47.86</td>\n",
       "      <td>52.81</td>\n",
       "      <td>47.50</td>\n",
       "      <td>58.42</td>\n",
       "      <td>56.53</td>\n",
       "      <td>54.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>6.25</td>\n",
       "      <td>48.14</td>\n",
       "      <td>5.29</td>\n",
       "      <td>62.73</td>\n",
       "      <td>9.45</td>\n",
       "      <td>64.43</td>\n",
       "      <td>60.80</td>\n",
       "      <td>62.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">participants</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.61</td>\n",
       "      <td>6.18</td>\n",
       "      <td>11.41</td>\n",
       "      <td>3.71</td>\n",
       "      <td>10.85</td>\n",
       "      <td>10.57</td>\n",
       "      <td>13.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>10.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.28</td>\n",
       "      <td>10.43</td>\n",
       "      <td>1.39</td>\n",
       "      <td>12.90</td>\n",
       "      <td>14.40</td>\n",
       "      <td>16.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.53</td>\n",
       "      <td>21.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.39</td>\n",
       "      <td>15.95</td>\n",
       "      <td>17.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>9.28</td>\n",
       "      <td>13.39</td>\n",
       "      <td>8.96</td>\n",
       "      <td>14.09</td>\n",
       "      <td>10.80</td>\n",
       "      <td>8.76</td>\n",
       "      <td>16.32</td>\n",
       "      <td>16.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>8.53</td>\n",
       "      <td>3.45</td>\n",
       "      <td>11.81</td>\n",
       "      <td>16.51</td>\n",
       "      <td>13.59</td>\n",
       "      <td>19.11</td>\n",
       "      <td>16.70</td>\n",
       "      <td>16.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>5.92</td>\n",
       "      <td>16.43</td>\n",
       "      <td>7.39</td>\n",
       "      <td>22.51</td>\n",
       "      <td>14.69</td>\n",
       "      <td>29.66</td>\n",
       "      <td>20.77</td>\n",
       "      <td>21.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>16.97</td>\n",
       "      <td>19.48</td>\n",
       "      <td>15.72</td>\n",
       "      <td>27.61</td>\n",
       "      <td>18.66</td>\n",
       "      <td>26.67</td>\n",
       "      <td>34.70</td>\n",
       "      <td>32.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>12.10</td>\n",
       "      <td>19.12</td>\n",
       "      <td>13.25</td>\n",
       "      <td>36.91</td>\n",
       "      <td>20.44</td>\n",
       "      <td>47.58</td>\n",
       "      <td>40.37</td>\n",
       "      <td>43.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>8.06</td>\n",
       "      <td>17.16</td>\n",
       "      <td>7.49</td>\n",
       "      <td>52.04</td>\n",
       "      <td>18.45</td>\n",
       "      <td>53.37</td>\n",
       "      <td>50.09</td>\n",
       "      <td>49.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">time expressions</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>2.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.45</td>\n",
       "      <td>9.90</td>\n",
       "      <td>5.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>7.71</td>\n",
       "      <td>5.48</td>\n",
       "      <td>10.03</td>\n",
       "      <td>6.55</td>\n",
       "      <td>8.70</td>\n",
       "      <td>11.86</td>\n",
       "      <td>11.87</td>\n",
       "      <td>12.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>6.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>33.33</td>\n",
       "      <td>6.55</td>\n",
       "      <td>30.67</td>\n",
       "      <td>22.02</td>\n",
       "      <td>4.61</td>\n",
       "      <td>20.79</td>\n",
       "      <td>17.60</td>\n",
       "      <td>20.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>34.23</td>\n",
       "      <td>16.59</td>\n",
       "      <td>29.06</td>\n",
       "      <td>37.10</td>\n",
       "      <td>11.83</td>\n",
       "      <td>28.17</td>\n",
       "      <td>22.86</td>\n",
       "      <td>23.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>38.46</td>\n",
       "      <td>23.42</td>\n",
       "      <td>38.46</td>\n",
       "      <td>42.34</td>\n",
       "      <td>24.14</td>\n",
       "      <td>38.96</td>\n",
       "      <td>32.75</td>\n",
       "      <td>33.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>44.94</td>\n",
       "      <td>38.02</td>\n",
       "      <td>44.64</td>\n",
       "      <td>51.69</td>\n",
       "      <td>40.00</td>\n",
       "      <td>47.06</td>\n",
       "      <td>44.63</td>\n",
       "      <td>40.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>46.46</td>\n",
       "      <td>39.46</td>\n",
       "      <td>46.02</td>\n",
       "      <td>41.58</td>\n",
       "      <td>43.33</td>\n",
       "      <td>39.53</td>\n",
       "      <td>31.95</td>\n",
       "      <td>41.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>42.55</td>\n",
       "      <td>57.73</td>\n",
       "      <td>43.56</td>\n",
       "      <td>65.93</td>\n",
       "      <td>51.49</td>\n",
       "      <td>57.43</td>\n",
       "      <td>61.22</td>\n",
       "      <td>58.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                f1                         \\\n",
       "template                                       ext    cls ext_def ext_exp   \n",
       "language   entity           model                                           \n",
       "English    event triggers   llama2-7b         0.00   1.34    0.00   10.30   \n",
       "                            llama2-7b-chat    6.41   0.00   10.90   28.80   \n",
       "                            llama2-13b        0.00   1.91    2.93    8.85   \n",
       "                            llama2-13b-chat   2.28   0.31    2.43   20.90   \n",
       "                            llama2-70b        8.34   3.31   14.96   26.44   \n",
       "                            llama2-70b-chat   1.17   3.30    2.29   41.22   \n",
       "                            gpt3              0.30   3.92    3.04   41.12   \n",
       "                            chatgpt           8.29  33.93   35.96   55.08   \n",
       "                            gpt4             20.45  57.82   21.36   72.81   \n",
       "           participants     llama2-7b          NaN    NaN     NaN     NaN   \n",
       "                            llama2-7b-chat     NaN    NaN     NaN     NaN   \n",
       "                            llama2-13b         NaN    NaN     NaN     NaN   \n",
       "                            llama2-13b-chat    NaN    NaN     NaN     NaN   \n",
       "                            llama2-70b         NaN    NaN     NaN     NaN   \n",
       "                            llama2-70b-chat    NaN    NaN     NaN     NaN   \n",
       "                            gpt3               NaN    NaN     NaN     NaN   \n",
       "                            chatgpt            NaN    NaN     NaN     NaN   \n",
       "                            gpt4               NaN    NaN     NaN     NaN   \n",
       "           time expressions llama2-7b         0.00   0.00    0.00    2.12   \n",
       "                            llama2-7b-chat    7.07   0.00   11.84   10.63   \n",
       "                            llama2-13b        1.32   0.00    3.30    2.27   \n",
       "                            llama2-13b-chat  21.09   8.99   22.22   16.62   \n",
       "                            llama2-70b       12.26   2.91   15.91   21.93   \n",
       "                            llama2-70b-chat  32.56  16.04   31.16   25.77   \n",
       "                            gpt3             37.61  27.01   39.17   37.84   \n",
       "                            chatgpt          35.82  26.38   30.61   42.64   \n",
       "                            gpt4             58.45  64.19   53.81   64.22   \n",
       "Portuguese event triggers   llama2-7b         0.00   0.00    9.30   29.64   \n",
       "                            llama2-7b-chat    3.49   0.00    9.27   24.20   \n",
       "                            llama2-13b        0.00   2.57   17.56   33.37   \n",
       "                            llama2-13b-chat   9.12   4.78   13.27   18.21   \n",
       "                            llama2-70b        3.76   0.78    7.73   31.75   \n",
       "                            llama2-70b-chat   2.60  10.84    1.57   30.04   \n",
       "                            gpt3              2.26  22.42    2.80   50.13   \n",
       "                            chatgpt          33.16  37.91   47.86   52.81   \n",
       "                            gpt4              6.25  48.14    5.29   62.73   \n",
       "           participants     llama2-7b         0.00   1.61    6.18   11.41   \n",
       "                            llama2-7b-chat   10.57   0.00   13.28   10.43   \n",
       "                            llama2-13b        0.00   0.00   10.53   21.33   \n",
       "                            llama2-13b-chat   9.28  13.39    8.96   14.09   \n",
       "                            llama2-70b        8.53   3.45   11.81   16.51   \n",
       "                            llama2-70b-chat   5.92  16.43    7.39   22.51   \n",
       "                            gpt3             16.97  19.48   15.72   27.61   \n",
       "                            chatgpt          12.10  19.12   13.25   36.91   \n",
       "                            gpt4              8.06  17.16    7.49   52.04   \n",
       "           time expressions llama2-7b         2.86   0.00    0.00    0.00   \n",
       "                            llama2-7b-chat    7.71   5.48   10.03    6.55   \n",
       "                            llama2-13b        6.45   0.00    4.11    0.00   \n",
       "                            llama2-13b-chat  33.33   6.55   30.67   22.02   \n",
       "                            llama2-70b       34.23  16.59   29.06   37.10   \n",
       "                            llama2-70b-chat  38.46  23.42   38.46   42.34   \n",
       "                            gpt3             44.94  38.02   44.64   51.69   \n",
       "                            chatgpt          46.46  39.46   46.02   41.58   \n",
       "                            gpt4             42.55  57.73   43.56   65.93   \n",
       "\n",
       "                                                                         \\\n",
       "template                                    cls_def ext_def_exp cls_exp   \n",
       "language   entity           model                                         \n",
       "English    event triggers   llama2-7b          0.00       15.40   11.06   \n",
       "                            llama2-7b-chat     0.00       31.15   28.30   \n",
       "                            llama2-13b         0.00       10.53    6.28   \n",
       "                            llama2-13b-chat    0.30       16.79   18.68   \n",
       "                            llama2-70b         3.10       25.45   13.77   \n",
       "                            llama2-70b-chat    4.95       42.13   18.12   \n",
       "                            gpt3               4.85       42.12   24.59   \n",
       "                            chatgpt           40.60       56.32   57.30   \n",
       "                            gpt4              33.33       72.93   72.60   \n",
       "           participants     llama2-7b           NaN         NaN     NaN   \n",
       "                            llama2-7b-chat      NaN         NaN     NaN   \n",
       "                            llama2-13b          NaN         NaN     NaN   \n",
       "                            llama2-13b-chat     NaN         NaN     NaN   \n",
       "                            llama2-70b          NaN         NaN     NaN   \n",
       "                            llama2-70b-chat     NaN         NaN     NaN   \n",
       "                            gpt3                NaN         NaN     NaN   \n",
       "                            chatgpt             NaN         NaN     NaN   \n",
       "                            gpt4                NaN         NaN     NaN   \n",
       "           time expressions llama2-7b          0.00        2.37    1.65   \n",
       "                            llama2-7b-chat     3.24        8.79    8.51   \n",
       "                            llama2-13b         0.00        1.49    0.00   \n",
       "                            llama2-13b-chat    9.37       13.75   12.11   \n",
       "                            llama2-70b         4.56       21.49   11.31   \n",
       "                            llama2-70b-chat   19.73       20.27   16.94   \n",
       "                            gpt3              21.57       40.30   31.61   \n",
       "                            chatgpt           29.59       47.57   36.91   \n",
       "                            gpt4              65.52       60.63   64.52   \n",
       "Portuguese event triggers   llama2-7b          3.09       27.68   19.32   \n",
       "                            llama2-7b-chat     0.00       25.88   27.03   \n",
       "                            llama2-13b         3.19       34.33   13.50   \n",
       "                            llama2-13b-chat    6.08       13.76   27.24   \n",
       "                            llama2-70b         9.86       31.31   30.53   \n",
       "                            llama2-70b-chat    9.31       32.14   30.85   \n",
       "                            gpt3              27.42       48.91   49.67   \n",
       "                            chatgpt           47.50       58.42   56.53   \n",
       "                            gpt4               9.45       64.43   60.80   \n",
       "           participants     llama2-7b          3.71       10.85   10.57   \n",
       "                            llama2-7b-chat     1.39       12.90   14.40   \n",
       "                            llama2-13b         0.00       18.39   15.95   \n",
       "                            llama2-13b-chat   10.80        8.76   16.32   \n",
       "                            llama2-70b        13.59       19.11   16.70   \n",
       "                            llama2-70b-chat   14.69       29.66   20.77   \n",
       "                            gpt3              18.66       26.67   34.70   \n",
       "                            chatgpt           20.44       47.58   40.37   \n",
       "                            gpt4              18.45       53.37   50.09   \n",
       "           time expressions llama2-7b          0.98        3.45    9.90   \n",
       "                            llama2-7b-chat     8.70       11.86   11.87   \n",
       "                            llama2-13b         0.00        3.92    3.23   \n",
       "                            llama2-13b-chat    4.61       20.79   17.60   \n",
       "                            llama2-70b        11.83       28.17   22.86   \n",
       "                            llama2-70b-chat   24.14       38.96   32.75   \n",
       "                            gpt3              40.00       47.06   44.63   \n",
       "                            chatgpt           43.33       39.53   31.95   \n",
       "                            gpt4              51.49       57.43   61.22   \n",
       "\n",
       "                                                         \n",
       "template                                    cls_def_exp  \n",
       "language   entity           model                        \n",
       "English    event triggers   llama2-7b              7.75  \n",
       "                            llama2-7b-chat        29.67  \n",
       "                            llama2-13b             2.10  \n",
       "                            llama2-13b-chat       18.15  \n",
       "                            llama2-70b            26.29  \n",
       "                            llama2-70b-chat       18.74  \n",
       "                            gpt3                  38.01  \n",
       "                            chatgpt               59.28  \n",
       "                            gpt4                  74.68  \n",
       "           participants     llama2-7b               NaN  \n",
       "                            llama2-7b-chat          NaN  \n",
       "                            llama2-13b              NaN  \n",
       "                            llama2-13b-chat         NaN  \n",
       "                            llama2-70b              NaN  \n",
       "                            llama2-70b-chat         NaN  \n",
       "                            gpt3                    NaN  \n",
       "                            chatgpt                 NaN  \n",
       "                            gpt4                    NaN  \n",
       "           time expressions llama2-7b              1.84  \n",
       "                            llama2-7b-chat         8.31  \n",
       "                            llama2-13b             0.00  \n",
       "                            llama2-13b-chat       16.36  \n",
       "                            llama2-70b            12.04  \n",
       "                            llama2-70b-chat       18.32  \n",
       "                            gpt3                  28.22  \n",
       "                            chatgpt               38.13  \n",
       "                            gpt4                  61.28  \n",
       "Portuguese event triggers   llama2-7b             19.51  \n",
       "                            llama2-7b-chat        23.91  \n",
       "                            llama2-13b            17.99  \n",
       "                            llama2-13b-chat       28.19  \n",
       "                            llama2-70b            29.67  \n",
       "                            llama2-70b-chat       33.50  \n",
       "                            gpt3                  49.57  \n",
       "                            chatgpt               54.03  \n",
       "                            gpt4                  62.40  \n",
       "           participants     llama2-7b             13.46  \n",
       "                            llama2-7b-chat        16.17  \n",
       "                            llama2-13b            17.35  \n",
       "                            llama2-13b-chat       16.23  \n",
       "                            llama2-70b            16.27  \n",
       "                            llama2-70b-chat       21.68  \n",
       "                            gpt3                  32.70  \n",
       "                            chatgpt               43.79  \n",
       "                            gpt4                  49.06  \n",
       "           time expressions llama2-7b              5.45  \n",
       "                            llama2-7b-chat        12.84  \n",
       "                            llama2-13b             3.39  \n",
       "                            llama2-13b-chat       20.44  \n",
       "                            llama2-70b            23.70  \n",
       "                            llama2-70b-chat       33.90  \n",
       "                            gpt3                  40.34  \n",
       "                            chatgpt               41.32  \n",
       "                            gpt4                  58.25  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[[\"language\", \"model\", \"template\", \"entity\", \"f1\"]].groupby(\n",
    "    [\"language\", \"entity\", \"model\", \"template\"]).mean(\"f1\")\n",
    "data = data.unstack(\"template\")\n",
    "data = data[[('f1', \"ext\"),\n",
    "            ('f1', \"cls\"),\n",
    "            ('f1', \"ext_def\"),\n",
    "            ('f1', \"ext_exp\"),\n",
    "            ('f1', \"cls_def\"),\n",
    "            ('f1', \"ext_def_exp\"),\n",
    "            ('f1', \"cls_exp\"),\n",
    "            ('f1', \"cls_def_exp\")]]\n",
    "(data * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = ROOT / \"results\" / \"test\"\n",
    "\n",
    "models = [\n",
    "    \"tei2go\",\n",
    "    \"heideltime\",\n",
    "    \"tefe\",\n",
    "    \"srl\",\n",
    "    \"ner\",\n",
    "    \"tieval_baseline\",\n",
    "    'llama2-7b',\n",
    "    'llama2-7b-chat',\n",
    "    'llama2-13b',\n",
    "    'llama2-13b-chat',\n",
    "    'llama2-70b',\n",
    "    'llama2-70b-chat',\n",
    "    'gpt3',\n",
    "    'chatgpt',\n",
    "    'gpt4',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt = pd.read_csv(results_path / \"portuguese\"/ \"results.csv\")\n",
    "df_pt[\"language\"] = \"Portuguese\"\n",
    "\n",
    "df_en = pd.read_csv(results_path / \"english\"/ \"results.csv\")\n",
    "df_en[\"language\"] = \"English\"\n",
    "\n",
    "df = pd.concat([df_pt, df_en])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df[\"model\"] = pd.Categorical(df.model, ordered=True, categories=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_r</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th>entity</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"45\" valign=\"top\">English</th>\n",
       "      <th rowspan=\"15\" valign=\"top\">event triggers</th>\n",
       "      <th>tei2go</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heideltime</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tefe</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srl</th>\n",
       "      <td>74.52</td>\n",
       "      <td>73.83</td>\n",
       "      <td>74.17</td>\n",
       "      <td>73.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tieval_baseline</th>\n",
       "      <td>65.37</td>\n",
       "      <td>65.48</td>\n",
       "      <td>65.43</td>\n",
       "      <td>63.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>21.74</td>\n",
       "      <td>12.50</td>\n",
       "      <td>15.87</td>\n",
       "      <td>11.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>31.20</td>\n",
       "      <td>27.46</td>\n",
       "      <td>29.21</td>\n",
       "      <td>36.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>22.16</td>\n",
       "      <td>11.07</td>\n",
       "      <td>14.77</td>\n",
       "      <td>7.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>20.30</td>\n",
       "      <td>22.92</td>\n",
       "      <td>21.53</td>\n",
       "      <td>22.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>21.93</td>\n",
       "      <td>26.17</td>\n",
       "      <td>23.86</td>\n",
       "      <td>21.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>42.77</td>\n",
       "      <td>41.76</td>\n",
       "      <td>42.26</td>\n",
       "      <td>51.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>58.79</td>\n",
       "      <td>38.29</td>\n",
       "      <td>46.37</td>\n",
       "      <td>58.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>79.30</td>\n",
       "      <td>36.65</td>\n",
       "      <td>50.14</td>\n",
       "      <td>71.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>81.43</td>\n",
       "      <td>57.27</td>\n",
       "      <td>67.25</td>\n",
       "      <td>77.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">participants</th>\n",
       "      <th>tei2go</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heideltime</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tefe</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srl</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tieval_baseline</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">time expressions</th>\n",
       "      <th>tei2go</th>\n",
       "      <td>87.07</td>\n",
       "      <td>71.44</td>\n",
       "      <td>78.49</td>\n",
       "      <td>88.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heideltime</th>\n",
       "      <td>85.90</td>\n",
       "      <td>70.72</td>\n",
       "      <td>77.57</td>\n",
       "      <td>87.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tefe</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srl</th>\n",
       "      <td>28.74</td>\n",
       "      <td>24.03</td>\n",
       "      <td>26.17</td>\n",
       "      <td>59.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tieval_baseline</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>2.42</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>11.12</td>\n",
       "      <td>18.22</td>\n",
       "      <td>13.81</td>\n",
       "      <td>23.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>3.99</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>30.89</td>\n",
       "      <td>30.19</td>\n",
       "      <td>30.54</td>\n",
       "      <td>46.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>20.89</td>\n",
       "      <td>21.21</td>\n",
       "      <td>21.05</td>\n",
       "      <td>39.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>33.66</td>\n",
       "      <td>37.17</td>\n",
       "      <td>35.33</td>\n",
       "      <td>54.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>40.13</td>\n",
       "      <td>40.71</td>\n",
       "      <td>40.41</td>\n",
       "      <td>58.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>55.54</td>\n",
       "      <td>36.36</td>\n",
       "      <td>43.95</td>\n",
       "      <td>61.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>69.71</td>\n",
       "      <td>58.84</td>\n",
       "      <td>63.82</td>\n",
       "      <td>81.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"45\" valign=\"top\">Portuguese</th>\n",
       "      <th rowspan=\"15\" valign=\"top\">event triggers</th>\n",
       "      <th>tei2go</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heideltime</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tefe</th>\n",
       "      <td>94.35</td>\n",
       "      <td>9.64</td>\n",
       "      <td>17.49</td>\n",
       "      <td>95.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srl</th>\n",
       "      <td>73.43</td>\n",
       "      <td>49.47</td>\n",
       "      <td>59.11</td>\n",
       "      <td>81.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tieval_baseline</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>23.84</td>\n",
       "      <td>37.61</td>\n",
       "      <td>29.18</td>\n",
       "      <td>21.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>24.82</td>\n",
       "      <td>25.44</td>\n",
       "      <td>25.13</td>\n",
       "      <td>30.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>31.46</td>\n",
       "      <td>33.61</td>\n",
       "      <td>32.50</td>\n",
       "      <td>28.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>26.16</td>\n",
       "      <td>29.62</td>\n",
       "      <td>27.78</td>\n",
       "      <td>35.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>22.41</td>\n",
       "      <td>34.24</td>\n",
       "      <td>27.09</td>\n",
       "      <td>27.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>26.73</td>\n",
       "      <td>40.14</td>\n",
       "      <td>32.09</td>\n",
       "      <td>32.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>41.21</td>\n",
       "      <td>53.11</td>\n",
       "      <td>46.41</td>\n",
       "      <td>50.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>52.65</td>\n",
       "      <td>60.04</td>\n",
       "      <td>56.10</td>\n",
       "      <td>64.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>61.25</td>\n",
       "      <td>66.83</td>\n",
       "      <td>63.92</td>\n",
       "      <td>68.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">participants</th>\n",
       "      <th>tei2go</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heideltime</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tefe</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srl</th>\n",
       "      <td>23.91</td>\n",
       "      <td>21.31</td>\n",
       "      <td>22.54</td>\n",
       "      <td>49.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner</th>\n",
       "      <td>32.97</td>\n",
       "      <td>2.94</td>\n",
       "      <td>5.41</td>\n",
       "      <td>55.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tieval_baseline</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>9.92</td>\n",
       "      <td>13.30</td>\n",
       "      <td>11.36</td>\n",
       "      <td>18.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>19.95</td>\n",
       "      <td>15.12</td>\n",
       "      <td>17.20</td>\n",
       "      <td>40.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>18.50</td>\n",
       "      <td>19.49</td>\n",
       "      <td>18.98</td>\n",
       "      <td>31.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>12.92</td>\n",
       "      <td>15.16</td>\n",
       "      <td>13.95</td>\n",
       "      <td>30.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>17.21</td>\n",
       "      <td>18.56</td>\n",
       "      <td>17.86</td>\n",
       "      <td>42.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>29.09</td>\n",
       "      <td>29.28</td>\n",
       "      <td>29.18</td>\n",
       "      <td>44.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>35.90</td>\n",
       "      <td>32.23</td>\n",
       "      <td>33.97</td>\n",
       "      <td>46.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>41.76</td>\n",
       "      <td>44.68</td>\n",
       "      <td>43.17</td>\n",
       "      <td>48.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>48.39</td>\n",
       "      <td>53.98</td>\n",
       "      <td>51.03</td>\n",
       "      <td>50.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">time expressions</th>\n",
       "      <th>tei2go</th>\n",
       "      <td>43.97</td>\n",
       "      <td>49.60</td>\n",
       "      <td>46.62</td>\n",
       "      <td>48.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heideltime</th>\n",
       "      <td>43.77</td>\n",
       "      <td>49.20</td>\n",
       "      <td>46.33</td>\n",
       "      <td>48.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tefe</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srl</th>\n",
       "      <td>21.05</td>\n",
       "      <td>28.80</td>\n",
       "      <td>24.32</td>\n",
       "      <td>50.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tieval_baseline</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>8.08</td>\n",
       "      <td>11.60</td>\n",
       "      <td>9.52</td>\n",
       "      <td>13.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>9.85</td>\n",
       "      <td>28.80</td>\n",
       "      <td>14.68</td>\n",
       "      <td>13.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>3.10</td>\n",
       "      <td>5.20</td>\n",
       "      <td>3.89</td>\n",
       "      <td>8.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>19.47</td>\n",
       "      <td>35.60</td>\n",
       "      <td>25.18</td>\n",
       "      <td>40.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>22.56</td>\n",
       "      <td>35.20</td>\n",
       "      <td>27.50</td>\n",
       "      <td>39.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>28.20</td>\n",
       "      <td>52.00</td>\n",
       "      <td>36.57</td>\n",
       "      <td>44.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>59.56</td>\n",
       "      <td>53.60</td>\n",
       "      <td>56.42</td>\n",
       "      <td>70.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>31.23</td>\n",
       "      <td>45.60</td>\n",
       "      <td>37.07</td>\n",
       "      <td>58.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>58.50</td>\n",
       "      <td>59.20</td>\n",
       "      <td>58.85</td>\n",
       "      <td>75.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             precision  recall     f1   f1_r\n",
       "language   entity           model                                           \n",
       "English    event triggers   tei2go                 NaN     NaN    NaN    NaN\n",
       "                            heideltime             NaN     NaN    NaN    NaN\n",
       "                            tefe                   NaN     NaN    NaN    NaN\n",
       "                            srl                  74.52   73.83  74.17  73.61\n",
       "                            ner                    NaN     NaN    NaN    NaN\n",
       "                            tieval_baseline      65.37   65.48  65.43  63.94\n",
       "                            llama2-7b            21.74   12.50  15.87  11.63\n",
       "                            llama2-7b-chat       31.20   27.46  29.21  36.78\n",
       "                            llama2-13b           22.16   11.07  14.77   7.83\n",
       "                            llama2-13b-chat      20.30   22.92  21.53  22.18\n",
       "                            llama2-70b           21.93   26.17  23.86  21.46\n",
       "                            llama2-70b-chat      42.77   41.76  42.26  51.19\n",
       "                            gpt3                 58.79   38.29  46.37  58.88\n",
       "                            chatgpt              79.30   36.65  50.14  71.56\n",
       "                            gpt4                 81.43   57.27  67.25  77.06\n",
       "           participants     tei2go                 NaN     NaN    NaN    NaN\n",
       "                            heideltime             NaN     NaN    NaN    NaN\n",
       "                            tefe                   NaN     NaN    NaN    NaN\n",
       "                            srl                   0.00    0.00   0.00   0.00\n",
       "                            ner                    NaN     NaN    NaN    NaN\n",
       "                            tieval_baseline        NaN     NaN    NaN    NaN\n",
       "                            llama2-7b              NaN     NaN    NaN    NaN\n",
       "                            llama2-7b-chat         NaN     NaN    NaN    NaN\n",
       "                            llama2-13b             NaN     NaN    NaN    NaN\n",
       "                            llama2-13b-chat        NaN     NaN    NaN    NaN\n",
       "                            llama2-70b             NaN     NaN    NaN    NaN\n",
       "                            llama2-70b-chat        NaN     NaN    NaN    NaN\n",
       "                            gpt3                   NaN     NaN    NaN    NaN\n",
       "                            chatgpt                NaN     NaN    NaN    NaN\n",
       "                            gpt4                   NaN     NaN    NaN    NaN\n",
       "           time expressions tei2go               87.07   71.44  78.49  88.24\n",
       "                            heideltime           85.90   70.72  77.57  87.90\n",
       "                            tefe                   NaN     NaN    NaN    NaN\n",
       "                            srl                  28.74   24.03  26.17  59.29\n",
       "                            ner                    NaN     NaN    NaN    NaN\n",
       "                            tieval_baseline        NaN     NaN    NaN    NaN\n",
       "                            llama2-7b             2.42    3.26   2.78   2.86\n",
       "                            llama2-7b-chat       11.12   18.22  13.81  23.51\n",
       "                            llama2-13b            3.99    1.36   2.03   5.65\n",
       "                            llama2-13b-chat      30.89   30.19  30.54  46.49\n",
       "                            llama2-70b           20.89   21.21  21.05  39.54\n",
       "                            llama2-70b-chat      33.66   37.17  35.33  54.80\n",
       "                            gpt3                 40.13   40.71  40.41  58.87\n",
       "                            chatgpt              55.54   36.36  43.95  61.90\n",
       "                            gpt4                 69.71   58.84  63.82  81.93\n",
       "Portuguese event triggers   tei2go                 NaN     NaN    NaN    NaN\n",
       "                            heideltime             NaN     NaN    NaN    NaN\n",
       "                            tefe                 94.35    9.64  17.49  95.08\n",
       "                            srl                  73.43   49.47  59.11  81.15\n",
       "                            ner                    NaN     NaN    NaN    NaN\n",
       "                            tieval_baseline        NaN     NaN    NaN    NaN\n",
       "                            llama2-7b            23.84   37.61  29.18  21.84\n",
       "                            llama2-7b-chat       24.82   25.44  25.13  30.07\n",
       "                            llama2-13b           31.46   33.61  32.50  28.16\n",
       "                            llama2-13b-chat      26.16   29.62  27.78  35.59\n",
       "                            llama2-70b           22.41   34.24  27.09  27.60\n",
       "                            llama2-70b-chat      26.73   40.14  32.09  32.12\n",
       "                            gpt3                 41.21   53.11  46.41  50.30\n",
       "                            chatgpt              52.65   60.04  56.10  64.06\n",
       "                            gpt4                 61.25   66.83  63.92  68.87\n",
       "           participants     tei2go                 NaN     NaN    NaN    NaN\n",
       "                            heideltime             NaN     NaN    NaN    NaN\n",
       "                            tefe                   NaN     NaN    NaN    NaN\n",
       "                            srl                  23.91   21.31  22.54  49.75\n",
       "                            ner                  32.97    2.94   5.41  55.67\n",
       "                            tieval_baseline        NaN     NaN    NaN    NaN\n",
       "                            llama2-7b             9.92   13.30  11.36  18.31\n",
       "                            llama2-7b-chat       19.95   15.12  17.20  40.25\n",
       "                            llama2-13b           18.50   19.49  18.98  31.15\n",
       "                            llama2-13b-chat      12.92   15.16  13.95  30.61\n",
       "                            llama2-70b           17.21   18.56  17.86  42.74\n",
       "                            llama2-70b-chat      29.09   29.28  29.18  44.78\n",
       "                            gpt3                 35.90   32.23  33.97  46.36\n",
       "                            chatgpt              41.76   44.68  43.17  48.16\n",
       "                            gpt4                 48.39   53.98  51.03  50.78\n",
       "           time expressions tei2go               43.97   49.60  46.62  48.73\n",
       "                            heideltime           43.77   49.20  46.33  48.91\n",
       "                            tefe                   NaN     NaN    NaN    NaN\n",
       "                            srl                  21.05   28.80  24.32  50.48\n",
       "                            ner                    NaN     NaN    NaN    NaN\n",
       "                            tieval_baseline        NaN     NaN    NaN    NaN\n",
       "                            llama2-7b             8.08   11.60   9.52  13.32\n",
       "                            llama2-7b-chat        9.85   28.80  14.68  13.61\n",
       "                            llama2-13b            3.10    5.20   3.89   8.78\n",
       "                            llama2-13b-chat      19.47   35.60  25.18  40.06\n",
       "                            llama2-70b           22.56   35.20  27.50  39.57\n",
       "                            llama2-70b-chat      28.20   52.00  36.57  44.05\n",
       "                            gpt3                 59.56   53.60  56.42  70.48\n",
       "                            chatgpt              31.23   45.60  37.07  58.22\n",
       "                            gpt4                 58.50   59.20  58.85  75.10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[[\"language\", \"entity\", \"model\", \"precision\", \"recall\", \"f1\", \"f1_r\"]]\n",
    "\n",
    "data = data.groupby([\"language\", \"entity\", \"model\"]).mean()\n",
    "(data * 100).round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

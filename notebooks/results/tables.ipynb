{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from src.reader import read_lusa, read_timebank\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial']\n",
    "\n",
    "\n",
    "ROOT = Path().resolve().parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'llama2-7b',\n",
    "    'llama2-7b-chat',\n",
    "    'llama2-13b',\n",
    "    'llama2-13b-chat',\n",
    "    'llama2-70b',\n",
    "    'llama2-70b-chat',\n",
    "    'gpt3',\n",
    "    'chatgpt',\n",
    "    'gpt4',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = ROOT / \"results\" / \"prompt_selection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>template</th>\n",
       "      <th>entity</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_r</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>ext</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.260331</td>\n",
       "      <td>0.331579</td>\n",
       "      <td>0.605356</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>cls</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.433511</td>\n",
       "      <td>0.336777</td>\n",
       "      <td>0.379070</td>\n",
       "      <td>0.617449</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>cls_def</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.601266</td>\n",
       "      <td>0.392562</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.784746</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.392562</td>\n",
       "      <td>0.478589</td>\n",
       "      <td>0.782263</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>ext_exp</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.490265</td>\n",
       "      <td>0.572314</td>\n",
       "      <td>0.528122</td>\n",
       "      <td>0.638241</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>llama2-7b-chat</td>\n",
       "      <td>cls_def_exp</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.083086</td>\n",
       "      <td>0.138516</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>llama2-7b-chat</td>\n",
       "      <td>cls_exp</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.067961</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.118924</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>llama2-7b-chat</td>\n",
       "      <td>ext_def_exp</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.066390</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.109416</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>llama2-7b-chat</td>\n",
       "      <td>ext_exp</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.106329</td>\n",
       "      <td>0.132826</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>llama2-7b-chat</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.095960</td>\n",
       "      <td>0.154472</td>\n",
       "      <td>0.118380</td>\n",
       "      <td>0.232903</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              model     template            entity  precision    recall  \\\n",
       "0           chatgpt          ext    event triggers   0.456522  0.260331   \n",
       "1           chatgpt          cls    event triggers   0.433511  0.336777   \n",
       "2           chatgpt      cls_def    event triggers   0.601266  0.392562   \n",
       "3           chatgpt      ext_def    event triggers   0.612903  0.392562   \n",
       "4           chatgpt      ext_exp    event triggers   0.490265  0.572314   \n",
       "..              ...          ...               ...        ...       ...   \n",
       "355  llama2-7b-chat  cls_def_exp  time expressions   0.065421  0.113821   \n",
       "356  llama2-7b-chat      cls_exp  time expressions   0.067961  0.113821   \n",
       "357  llama2-7b-chat  ext_def_exp  time expressions   0.066390  0.130081   \n",
       "358  llama2-7b-chat      ext_exp  time expressions   0.077206  0.170732   \n",
       "359  llama2-7b-chat      ext_def  time expressions   0.095960  0.154472   \n",
       "\n",
       "           f1      f1_r    language  \n",
       "0    0.331579  0.605356  Portuguese  \n",
       "1    0.379070  0.617449  Portuguese  \n",
       "2    0.475000  0.784746  Portuguese  \n",
       "3    0.478589  0.782263  Portuguese  \n",
       "4    0.528122  0.638241  Portuguese  \n",
       "..        ...       ...         ...  \n",
       "355  0.083086  0.138516     English  \n",
       "356  0.085106  0.118924     English  \n",
       "357  0.087912  0.109416     English  \n",
       "358  0.106329  0.132826     English  \n",
       "359  0.118380  0.232903     English  \n",
       "\n",
       "[360 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pt = pd.read_csv(results_path / \"portuguese\"/ \"results.csv\")\n",
    "df_pt[\"language\"] = \"Portuguese\"\n",
    "\n",
    "df_en = pd.read_csv(results_path / \"english\"/ \"results.csv\")\n",
    "df_en[\"language\"] = \"English\"\n",
    "\n",
    "df = pd.concat([df_pt, df_en])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df[\"model\"] = pd.Categorical(df.model, ordered=True, categories=models)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = df.language.unique().tolist()\n",
    "entities = df.entity.unique().tolist()\n",
    "templates = [\"ext\", \"cls\",\t\"ext_def\", \"ext_exp\", \"cls_def\", \"ext_def_exp\",  \"cls_exp\", \"cls_def_exp\"]\n",
    "labels = [\"_ _ _\", \"C _ _\", \"_ D _\", \"_ _ E\", \"C D _\", \"_ D E\",  \"C _ E\", \"C D E\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>template</th>\n",
       "      <th>ext</th>\n",
       "      <th>cls</th>\n",
       "      <th>ext_def</th>\n",
       "      <th>ext_exp</th>\n",
       "      <th>cls_def</th>\n",
       "      <th>ext_def_exp</th>\n",
       "      <th>cls_exp</th>\n",
       "      <th>cls_def_exp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th>entity</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"27\" valign=\"top\">English</th>\n",
       "      <th rowspan=\"9\" valign=\"top\">event triggers</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.40</td>\n",
       "      <td>11.06</td>\n",
       "      <td>7.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>6.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.90</td>\n",
       "      <td>28.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.15</td>\n",
       "      <td>28.30</td>\n",
       "      <td>29.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.91</td>\n",
       "      <td>2.93</td>\n",
       "      <td>8.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.53</td>\n",
       "      <td>6.28</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>2.28</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.43</td>\n",
       "      <td>20.90</td>\n",
       "      <td>0.30</td>\n",
       "      <td>16.79</td>\n",
       "      <td>18.68</td>\n",
       "      <td>18.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>8.34</td>\n",
       "      <td>3.31</td>\n",
       "      <td>14.96</td>\n",
       "      <td>26.44</td>\n",
       "      <td>3.10</td>\n",
       "      <td>25.45</td>\n",
       "      <td>13.77</td>\n",
       "      <td>26.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>1.17</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.29</td>\n",
       "      <td>41.22</td>\n",
       "      <td>4.95</td>\n",
       "      <td>42.13</td>\n",
       "      <td>18.12</td>\n",
       "      <td>18.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.04</td>\n",
       "      <td>41.12</td>\n",
       "      <td>4.85</td>\n",
       "      <td>42.12</td>\n",
       "      <td>24.59</td>\n",
       "      <td>38.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>8.29</td>\n",
       "      <td>33.93</td>\n",
       "      <td>35.96</td>\n",
       "      <td>55.08</td>\n",
       "      <td>40.60</td>\n",
       "      <td>56.32</td>\n",
       "      <td>57.30</td>\n",
       "      <td>59.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>20.45</td>\n",
       "      <td>57.82</td>\n",
       "      <td>21.36</td>\n",
       "      <td>72.81</td>\n",
       "      <td>33.33</td>\n",
       "      <td>72.93</td>\n",
       "      <td>72.60</td>\n",
       "      <td>74.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">participants</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">time expressions</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>7.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.84</td>\n",
       "      <td>10.63</td>\n",
       "      <td>3.24</td>\n",
       "      <td>8.79</td>\n",
       "      <td>8.51</td>\n",
       "      <td>8.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>1.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>21.09</td>\n",
       "      <td>8.99</td>\n",
       "      <td>22.22</td>\n",
       "      <td>16.62</td>\n",
       "      <td>9.37</td>\n",
       "      <td>13.75</td>\n",
       "      <td>12.11</td>\n",
       "      <td>16.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>12.26</td>\n",
       "      <td>2.91</td>\n",
       "      <td>15.91</td>\n",
       "      <td>21.93</td>\n",
       "      <td>4.56</td>\n",
       "      <td>21.49</td>\n",
       "      <td>11.31</td>\n",
       "      <td>12.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>32.56</td>\n",
       "      <td>16.04</td>\n",
       "      <td>31.16</td>\n",
       "      <td>25.77</td>\n",
       "      <td>19.73</td>\n",
       "      <td>20.27</td>\n",
       "      <td>16.94</td>\n",
       "      <td>18.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>37.61</td>\n",
       "      <td>27.01</td>\n",
       "      <td>39.17</td>\n",
       "      <td>37.84</td>\n",
       "      <td>21.57</td>\n",
       "      <td>40.30</td>\n",
       "      <td>31.61</td>\n",
       "      <td>28.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>35.82</td>\n",
       "      <td>26.38</td>\n",
       "      <td>30.61</td>\n",
       "      <td>42.64</td>\n",
       "      <td>29.59</td>\n",
       "      <td>47.57</td>\n",
       "      <td>36.91</td>\n",
       "      <td>38.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>58.45</td>\n",
       "      <td>64.19</td>\n",
       "      <td>53.81</td>\n",
       "      <td>64.22</td>\n",
       "      <td>65.52</td>\n",
       "      <td>60.63</td>\n",
       "      <td>64.52</td>\n",
       "      <td>61.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"27\" valign=\"top\">Portuguese</th>\n",
       "      <th rowspan=\"9\" valign=\"top\">event triggers</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.30</td>\n",
       "      <td>29.64</td>\n",
       "      <td>3.09</td>\n",
       "      <td>27.68</td>\n",
       "      <td>19.32</td>\n",
       "      <td>19.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>3.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.27</td>\n",
       "      <td>24.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.88</td>\n",
       "      <td>27.03</td>\n",
       "      <td>23.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.57</td>\n",
       "      <td>17.56</td>\n",
       "      <td>33.37</td>\n",
       "      <td>3.19</td>\n",
       "      <td>34.33</td>\n",
       "      <td>13.50</td>\n",
       "      <td>17.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>9.12</td>\n",
       "      <td>4.78</td>\n",
       "      <td>13.27</td>\n",
       "      <td>18.21</td>\n",
       "      <td>6.08</td>\n",
       "      <td>13.76</td>\n",
       "      <td>27.24</td>\n",
       "      <td>28.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>3.76</td>\n",
       "      <td>0.78</td>\n",
       "      <td>7.73</td>\n",
       "      <td>31.75</td>\n",
       "      <td>9.86</td>\n",
       "      <td>31.31</td>\n",
       "      <td>30.53</td>\n",
       "      <td>29.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>2.60</td>\n",
       "      <td>10.84</td>\n",
       "      <td>1.57</td>\n",
       "      <td>30.04</td>\n",
       "      <td>9.31</td>\n",
       "      <td>32.14</td>\n",
       "      <td>30.85</td>\n",
       "      <td>33.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>2.26</td>\n",
       "      <td>22.42</td>\n",
       "      <td>2.80</td>\n",
       "      <td>50.13</td>\n",
       "      <td>27.42</td>\n",
       "      <td>48.91</td>\n",
       "      <td>49.67</td>\n",
       "      <td>49.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>33.16</td>\n",
       "      <td>37.91</td>\n",
       "      <td>47.86</td>\n",
       "      <td>52.81</td>\n",
       "      <td>47.50</td>\n",
       "      <td>58.42</td>\n",
       "      <td>56.53</td>\n",
       "      <td>54.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>6.25</td>\n",
       "      <td>48.14</td>\n",
       "      <td>5.29</td>\n",
       "      <td>62.73</td>\n",
       "      <td>9.45</td>\n",
       "      <td>64.43</td>\n",
       "      <td>60.80</td>\n",
       "      <td>62.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">participants</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.61</td>\n",
       "      <td>6.18</td>\n",
       "      <td>11.41</td>\n",
       "      <td>3.71</td>\n",
       "      <td>10.85</td>\n",
       "      <td>10.57</td>\n",
       "      <td>13.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>10.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.28</td>\n",
       "      <td>10.43</td>\n",
       "      <td>1.39</td>\n",
       "      <td>12.90</td>\n",
       "      <td>14.40</td>\n",
       "      <td>16.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.53</td>\n",
       "      <td>21.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.39</td>\n",
       "      <td>15.95</td>\n",
       "      <td>17.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>9.28</td>\n",
       "      <td>13.39</td>\n",
       "      <td>8.96</td>\n",
       "      <td>14.09</td>\n",
       "      <td>10.80</td>\n",
       "      <td>8.76</td>\n",
       "      <td>16.32</td>\n",
       "      <td>16.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>8.53</td>\n",
       "      <td>3.45</td>\n",
       "      <td>11.81</td>\n",
       "      <td>16.51</td>\n",
       "      <td>13.59</td>\n",
       "      <td>19.11</td>\n",
       "      <td>16.70</td>\n",
       "      <td>16.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>5.92</td>\n",
       "      <td>16.43</td>\n",
       "      <td>7.39</td>\n",
       "      <td>22.51</td>\n",
       "      <td>14.69</td>\n",
       "      <td>29.66</td>\n",
       "      <td>20.77</td>\n",
       "      <td>21.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>16.97</td>\n",
       "      <td>19.48</td>\n",
       "      <td>15.72</td>\n",
       "      <td>27.61</td>\n",
       "      <td>18.66</td>\n",
       "      <td>26.67</td>\n",
       "      <td>34.70</td>\n",
       "      <td>32.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>12.10</td>\n",
       "      <td>19.12</td>\n",
       "      <td>13.25</td>\n",
       "      <td>36.91</td>\n",
       "      <td>20.44</td>\n",
       "      <td>47.58</td>\n",
       "      <td>40.37</td>\n",
       "      <td>43.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>8.06</td>\n",
       "      <td>17.16</td>\n",
       "      <td>7.49</td>\n",
       "      <td>52.04</td>\n",
       "      <td>18.45</td>\n",
       "      <td>53.37</td>\n",
       "      <td>50.09</td>\n",
       "      <td>49.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">time expressions</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>2.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.45</td>\n",
       "      <td>9.90</td>\n",
       "      <td>5.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>7.71</td>\n",
       "      <td>5.48</td>\n",
       "      <td>10.03</td>\n",
       "      <td>6.55</td>\n",
       "      <td>8.70</td>\n",
       "      <td>11.86</td>\n",
       "      <td>11.87</td>\n",
       "      <td>12.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>6.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>33.33</td>\n",
       "      <td>6.55</td>\n",
       "      <td>30.67</td>\n",
       "      <td>22.02</td>\n",
       "      <td>4.61</td>\n",
       "      <td>20.79</td>\n",
       "      <td>17.60</td>\n",
       "      <td>20.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>34.23</td>\n",
       "      <td>16.59</td>\n",
       "      <td>29.06</td>\n",
       "      <td>37.10</td>\n",
       "      <td>11.83</td>\n",
       "      <td>28.17</td>\n",
       "      <td>22.86</td>\n",
       "      <td>23.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>38.46</td>\n",
       "      <td>23.42</td>\n",
       "      <td>38.46</td>\n",
       "      <td>42.34</td>\n",
       "      <td>24.14</td>\n",
       "      <td>38.96</td>\n",
       "      <td>32.75</td>\n",
       "      <td>33.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>44.94</td>\n",
       "      <td>38.02</td>\n",
       "      <td>44.64</td>\n",
       "      <td>51.69</td>\n",
       "      <td>40.00</td>\n",
       "      <td>47.06</td>\n",
       "      <td>44.63</td>\n",
       "      <td>40.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>46.46</td>\n",
       "      <td>39.46</td>\n",
       "      <td>46.02</td>\n",
       "      <td>41.58</td>\n",
       "      <td>43.33</td>\n",
       "      <td>39.53</td>\n",
       "      <td>31.95</td>\n",
       "      <td>41.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>42.55</td>\n",
       "      <td>57.73</td>\n",
       "      <td>43.56</td>\n",
       "      <td>65.93</td>\n",
       "      <td>51.49</td>\n",
       "      <td>57.43</td>\n",
       "      <td>61.22</td>\n",
       "      <td>58.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                f1                         \\\n",
       "template                                       ext    cls ext_def ext_exp   \n",
       "language   entity           model                                           \n",
       "English    event triggers   llama2-7b         0.00   1.34    0.00   10.30   \n",
       "                            llama2-7b-chat    6.41   0.00   10.90   28.80   \n",
       "                            llama2-13b        0.00   1.91    2.93    8.85   \n",
       "                            llama2-13b-chat   2.28   0.31    2.43   20.90   \n",
       "                            llama2-70b        8.34   3.31   14.96   26.44   \n",
       "                            llama2-70b-chat   1.17   3.30    2.29   41.22   \n",
       "                            gpt3              0.30   3.92    3.04   41.12   \n",
       "                            chatgpt           8.29  33.93   35.96   55.08   \n",
       "                            gpt4             20.45  57.82   21.36   72.81   \n",
       "           participants     llama2-7b          NaN    NaN     NaN     NaN   \n",
       "                            llama2-7b-chat     NaN    NaN     NaN     NaN   \n",
       "                            llama2-13b         NaN    NaN     NaN     NaN   \n",
       "                            llama2-13b-chat    NaN    NaN     NaN     NaN   \n",
       "                            llama2-70b         NaN    NaN     NaN     NaN   \n",
       "                            llama2-70b-chat    NaN    NaN     NaN     NaN   \n",
       "                            gpt3               NaN    NaN     NaN     NaN   \n",
       "                            chatgpt            NaN    NaN     NaN     NaN   \n",
       "                            gpt4               NaN    NaN     NaN     NaN   \n",
       "           time expressions llama2-7b         0.00   0.00    0.00    2.12   \n",
       "                            llama2-7b-chat    7.07   0.00   11.84   10.63   \n",
       "                            llama2-13b        1.32   0.00    3.30    2.27   \n",
       "                            llama2-13b-chat  21.09   8.99   22.22   16.62   \n",
       "                            llama2-70b       12.26   2.91   15.91   21.93   \n",
       "                            llama2-70b-chat  32.56  16.04   31.16   25.77   \n",
       "                            gpt3             37.61  27.01   39.17   37.84   \n",
       "                            chatgpt          35.82  26.38   30.61   42.64   \n",
       "                            gpt4             58.45  64.19   53.81   64.22   \n",
       "Portuguese event triggers   llama2-7b         0.00   0.00    9.30   29.64   \n",
       "                            llama2-7b-chat    3.49   0.00    9.27   24.20   \n",
       "                            llama2-13b        0.00   2.57   17.56   33.37   \n",
       "                            llama2-13b-chat   9.12   4.78   13.27   18.21   \n",
       "                            llama2-70b        3.76   0.78    7.73   31.75   \n",
       "                            llama2-70b-chat   2.60  10.84    1.57   30.04   \n",
       "                            gpt3              2.26  22.42    2.80   50.13   \n",
       "                            chatgpt          33.16  37.91   47.86   52.81   \n",
       "                            gpt4              6.25  48.14    5.29   62.73   \n",
       "           participants     llama2-7b         0.00   1.61    6.18   11.41   \n",
       "                            llama2-7b-chat   10.57   0.00   13.28   10.43   \n",
       "                            llama2-13b        0.00   0.00   10.53   21.33   \n",
       "                            llama2-13b-chat   9.28  13.39    8.96   14.09   \n",
       "                            llama2-70b        8.53   3.45   11.81   16.51   \n",
       "                            llama2-70b-chat   5.92  16.43    7.39   22.51   \n",
       "                            gpt3             16.97  19.48   15.72   27.61   \n",
       "                            chatgpt          12.10  19.12   13.25   36.91   \n",
       "                            gpt4              8.06  17.16    7.49   52.04   \n",
       "           time expressions llama2-7b         2.86   0.00    0.00    0.00   \n",
       "                            llama2-7b-chat    7.71   5.48   10.03    6.55   \n",
       "                            llama2-13b        6.45   0.00    4.11    0.00   \n",
       "                            llama2-13b-chat  33.33   6.55   30.67   22.02   \n",
       "                            llama2-70b       34.23  16.59   29.06   37.10   \n",
       "                            llama2-70b-chat  38.46  23.42   38.46   42.34   \n",
       "                            gpt3             44.94  38.02   44.64   51.69   \n",
       "                            chatgpt          46.46  39.46   46.02   41.58   \n",
       "                            gpt4             42.55  57.73   43.56   65.93   \n",
       "\n",
       "                                                                         \\\n",
       "template                                    cls_def ext_def_exp cls_exp   \n",
       "language   entity           model                                         \n",
       "English    event triggers   llama2-7b          0.00       15.40   11.06   \n",
       "                            llama2-7b-chat     0.00       31.15   28.30   \n",
       "                            llama2-13b         0.00       10.53    6.28   \n",
       "                            llama2-13b-chat    0.30       16.79   18.68   \n",
       "                            llama2-70b         3.10       25.45   13.77   \n",
       "                            llama2-70b-chat    4.95       42.13   18.12   \n",
       "                            gpt3               4.85       42.12   24.59   \n",
       "                            chatgpt           40.60       56.32   57.30   \n",
       "                            gpt4              33.33       72.93   72.60   \n",
       "           participants     llama2-7b           NaN         NaN     NaN   \n",
       "                            llama2-7b-chat      NaN         NaN     NaN   \n",
       "                            llama2-13b          NaN         NaN     NaN   \n",
       "                            llama2-13b-chat     NaN         NaN     NaN   \n",
       "                            llama2-70b          NaN         NaN     NaN   \n",
       "                            llama2-70b-chat     NaN         NaN     NaN   \n",
       "                            gpt3                NaN         NaN     NaN   \n",
       "                            chatgpt             NaN         NaN     NaN   \n",
       "                            gpt4                NaN         NaN     NaN   \n",
       "           time expressions llama2-7b          0.00        2.37    1.65   \n",
       "                            llama2-7b-chat     3.24        8.79    8.51   \n",
       "                            llama2-13b         0.00        1.49    0.00   \n",
       "                            llama2-13b-chat    9.37       13.75   12.11   \n",
       "                            llama2-70b         4.56       21.49   11.31   \n",
       "                            llama2-70b-chat   19.73       20.27   16.94   \n",
       "                            gpt3              21.57       40.30   31.61   \n",
       "                            chatgpt           29.59       47.57   36.91   \n",
       "                            gpt4              65.52       60.63   64.52   \n",
       "Portuguese event triggers   llama2-7b          3.09       27.68   19.32   \n",
       "                            llama2-7b-chat     0.00       25.88   27.03   \n",
       "                            llama2-13b         3.19       34.33   13.50   \n",
       "                            llama2-13b-chat    6.08       13.76   27.24   \n",
       "                            llama2-70b         9.86       31.31   30.53   \n",
       "                            llama2-70b-chat    9.31       32.14   30.85   \n",
       "                            gpt3              27.42       48.91   49.67   \n",
       "                            chatgpt           47.50       58.42   56.53   \n",
       "                            gpt4               9.45       64.43   60.80   \n",
       "           participants     llama2-7b          3.71       10.85   10.57   \n",
       "                            llama2-7b-chat     1.39       12.90   14.40   \n",
       "                            llama2-13b         0.00       18.39   15.95   \n",
       "                            llama2-13b-chat   10.80        8.76   16.32   \n",
       "                            llama2-70b        13.59       19.11   16.70   \n",
       "                            llama2-70b-chat   14.69       29.66   20.77   \n",
       "                            gpt3              18.66       26.67   34.70   \n",
       "                            chatgpt           20.44       47.58   40.37   \n",
       "                            gpt4              18.45       53.37   50.09   \n",
       "           time expressions llama2-7b          0.98        3.45    9.90   \n",
       "                            llama2-7b-chat     8.70       11.86   11.87   \n",
       "                            llama2-13b         0.00        3.92    3.23   \n",
       "                            llama2-13b-chat    4.61       20.79   17.60   \n",
       "                            llama2-70b        11.83       28.17   22.86   \n",
       "                            llama2-70b-chat   24.14       38.96   32.75   \n",
       "                            gpt3              40.00       47.06   44.63   \n",
       "                            chatgpt           43.33       39.53   31.95   \n",
       "                            gpt4              51.49       57.43   61.22   \n",
       "\n",
       "                                                         \n",
       "template                                    cls_def_exp  \n",
       "language   entity           model                        \n",
       "English    event triggers   llama2-7b              7.75  \n",
       "                            llama2-7b-chat        29.67  \n",
       "                            llama2-13b             2.10  \n",
       "                            llama2-13b-chat       18.15  \n",
       "                            llama2-70b            26.29  \n",
       "                            llama2-70b-chat       18.74  \n",
       "                            gpt3                  38.01  \n",
       "                            chatgpt               59.28  \n",
       "                            gpt4                  74.68  \n",
       "           participants     llama2-7b               NaN  \n",
       "                            llama2-7b-chat          NaN  \n",
       "                            llama2-13b              NaN  \n",
       "                            llama2-13b-chat         NaN  \n",
       "                            llama2-70b              NaN  \n",
       "                            llama2-70b-chat         NaN  \n",
       "                            gpt3                    NaN  \n",
       "                            chatgpt                 NaN  \n",
       "                            gpt4                    NaN  \n",
       "           time expressions llama2-7b              1.84  \n",
       "                            llama2-7b-chat         8.31  \n",
       "                            llama2-13b             0.00  \n",
       "                            llama2-13b-chat       16.36  \n",
       "                            llama2-70b            12.04  \n",
       "                            llama2-70b-chat       18.32  \n",
       "                            gpt3                  28.22  \n",
       "                            chatgpt               38.13  \n",
       "                            gpt4                  61.28  \n",
       "Portuguese event triggers   llama2-7b             19.51  \n",
       "                            llama2-7b-chat        23.91  \n",
       "                            llama2-13b            17.99  \n",
       "                            llama2-13b-chat       28.19  \n",
       "                            llama2-70b            29.67  \n",
       "                            llama2-70b-chat       33.50  \n",
       "                            gpt3                  49.57  \n",
       "                            chatgpt               54.03  \n",
       "                            gpt4                  62.40  \n",
       "           participants     llama2-7b             13.46  \n",
       "                            llama2-7b-chat        16.17  \n",
       "                            llama2-13b            17.35  \n",
       "                            llama2-13b-chat       16.23  \n",
       "                            llama2-70b            16.27  \n",
       "                            llama2-70b-chat       21.68  \n",
       "                            gpt3                  32.70  \n",
       "                            chatgpt               43.79  \n",
       "                            gpt4                  49.06  \n",
       "           time expressions llama2-7b              5.45  \n",
       "                            llama2-7b-chat        12.84  \n",
       "                            llama2-13b             3.39  \n",
       "                            llama2-13b-chat       20.44  \n",
       "                            llama2-70b            23.70  \n",
       "                            llama2-70b-chat       33.90  \n",
       "                            gpt3                  40.34  \n",
       "                            chatgpt               41.32  \n",
       "                            gpt4                  58.25  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[[\"language\", \"model\", \"template\", \"entity\", \"f1\"]].groupby(\n",
    "    [\"language\", \"entity\", \"model\", \"template\"]).mean(\"f1\")\n",
    "data = data.unstack(\"template\")\n",
    "data = data[[('f1', \"ext\"),\n",
    "            ('f1', \"cls\"),\n",
    "            ('f1', \"ext_def\"),\n",
    "            ('f1', \"ext_exp\"),\n",
    "            ('f1', \"cls_def\"),\n",
    "            ('f1', \"ext_def_exp\"),\n",
    "            ('f1', \"cls_exp\"),\n",
    "            ('f1', \"cls_def_exp\")]]\n",
    "(data * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = ROOT / \"results\" / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>template</th>\n",
       "      <th>entity</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_r</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>cls_def_exp</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.487685</td>\n",
       "      <td>0.571492</td>\n",
       "      <td>0.526273</td>\n",
       "      <td>0.585791</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt3</td>\n",
       "      <td>cls_def_exp</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.439163</td>\n",
       "      <td>0.549734</td>\n",
       "      <td>0.488267</td>\n",
       "      <td>0.509444</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>cls_exp</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.597454</td>\n",
       "      <td>0.666963</td>\n",
       "      <td>0.630298</td>\n",
       "      <td>0.684470</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama2-13b</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.098135</td>\n",
       "      <td>0.130153</td>\n",
       "      <td>0.183512</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama2-13b-chat</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.205496</td>\n",
       "      <td>0.076377</td>\n",
       "      <td>0.111363</td>\n",
       "      <td>0.384928</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama2-70b</td>\n",
       "      <td>cls_def_exp</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.264538</td>\n",
       "      <td>0.309059</td>\n",
       "      <td>0.285071</td>\n",
       "      <td>0.273665</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama2-70b-chat</td>\n",
       "      <td>cls</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.168570</td>\n",
       "      <td>0.137655</td>\n",
       "      <td>0.151552</td>\n",
       "      <td>0.252957</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama2-7b</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.087687</td>\n",
       "      <td>0.041741</td>\n",
       "      <td>0.056558</td>\n",
       "      <td>0.306139</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama2-7b-chat</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.169525</td>\n",
       "      <td>0.114121</td>\n",
       "      <td>0.136412</td>\n",
       "      <td>0.246640</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>cls_def</td>\n",
       "      <td>participants</td>\n",
       "      <td>0.241742</td>\n",
       "      <td>0.130206</td>\n",
       "      <td>0.169251</td>\n",
       "      <td>0.619661</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt3</td>\n",
       "      <td>cls</td>\n",
       "      <td>participants</td>\n",
       "      <td>0.266939</td>\n",
       "      <td>0.132228</td>\n",
       "      <td>0.176852</td>\n",
       "      <td>0.603474</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>cls_def</td>\n",
       "      <td>participants</td>\n",
       "      <td>0.281757</td>\n",
       "      <td>0.168621</td>\n",
       "      <td>0.210979</td>\n",
       "      <td>0.661836</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>llama2-13b</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>participants</td>\n",
       "      <td>0.072396</td>\n",
       "      <td>0.069147</td>\n",
       "      <td>0.070734</td>\n",
       "      <td>0.263916</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>llama2-13b-chat</td>\n",
       "      <td>cls</td>\n",
       "      <td>participants</td>\n",
       "      <td>0.233454</td>\n",
       "      <td>0.078447</td>\n",
       "      <td>0.117433</td>\n",
       "      <td>0.530240</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>llama2-70b</td>\n",
       "      <td>cls_def_exp</td>\n",
       "      <td>participants</td>\n",
       "      <td>0.190665</td>\n",
       "      <td>0.155277</td>\n",
       "      <td>0.171161</td>\n",
       "      <td>0.341568</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>llama2-70b-chat</td>\n",
       "      <td>cls</td>\n",
       "      <td>participants</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>0.113223</td>\n",
       "      <td>0.158058</td>\n",
       "      <td>0.567567</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>llama2-7b</td>\n",
       "      <td>cls_def_exp</td>\n",
       "      <td>participants</td>\n",
       "      <td>0.099156</td>\n",
       "      <td>0.133037</td>\n",
       "      <td>0.113625</td>\n",
       "      <td>0.183143</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>llama2-7b-chat</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>participants</td>\n",
       "      <td>0.194226</td>\n",
       "      <td>0.089770</td>\n",
       "      <td>0.122788</td>\n",
       "      <td>0.473043</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>ext</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.312329</td>\n",
       "      <td>0.456000</td>\n",
       "      <td>0.370732</td>\n",
       "      <td>0.582215</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt3</td>\n",
       "      <td>ext_exp</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.595556</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.564211</td>\n",
       "      <td>0.704828</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>cls</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.488055</td>\n",
       "      <td>0.572000</td>\n",
       "      <td>0.526703</td>\n",
       "      <td>0.651046</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>llama2-13b</td>\n",
       "      <td>ext</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.031026</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.038864</td>\n",
       "      <td>0.087753</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>llama2-13b-chat</td>\n",
       "      <td>ext</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.194748</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>0.251768</td>\n",
       "      <td>0.400602</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>llama2-70b</td>\n",
       "      <td>ext_exp</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.225641</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.395740</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>llama2-70b-chat</td>\n",
       "      <td>ext_def_exp</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.268559</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.347458</td>\n",
       "      <td>0.426991</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>llama2-7b</td>\n",
       "      <td>ext</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>0.040563</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>llama2-7b-chat</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.061551</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.102598</td>\n",
       "      <td>0.125745</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>cls_def_exp</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.793842</td>\n",
       "      <td>0.391247</td>\n",
       "      <td>0.524160</td>\n",
       "      <td>0.720884</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>gpt3</td>\n",
       "      <td>cls_def</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.068923</td>\n",
       "      <td>0.061799</td>\n",
       "      <td>0.065167</td>\n",
       "      <td>0.178783</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>cls_exp</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.833139</td>\n",
       "      <td>0.628106</td>\n",
       "      <td>0.716238</td>\n",
       "      <td>0.819496</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>llama2-13b</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.114035</td>\n",
       "      <td>0.020013</td>\n",
       "      <td>0.034051</td>\n",
       "      <td>0.130065</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>llama2-13b-chat</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.091335</td>\n",
       "      <td>0.025731</td>\n",
       "      <td>0.040151</td>\n",
       "      <td>0.271489</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>llama2-70b</td>\n",
       "      <td>ext_def_exp</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.320189</td>\n",
       "      <td>0.253574</td>\n",
       "      <td>0.283014</td>\n",
       "      <td>0.262513</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>llama2-70b-chat</td>\n",
       "      <td>cls_def_exp</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.173713</td>\n",
       "      <td>0.149109</td>\n",
       "      <td>0.160473</td>\n",
       "      <td>0.142373</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>llama2-7b</td>\n",
       "      <td>cls</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013116</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>llama2-7b-chat</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>event triggers</td>\n",
       "      <td>0.151386</td>\n",
       "      <td>0.078073</td>\n",
       "      <td>0.103018</td>\n",
       "      <td>0.249106</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>ext_exp</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.559066</td>\n",
       "      <td>0.371689</td>\n",
       "      <td>0.446517</td>\n",
       "      <td>0.703927</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>gpt3</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.420996</td>\n",
       "      <td>0.355251</td>\n",
       "      <td>0.385339</td>\n",
       "      <td>0.643673</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>cls_exp</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.583410</td>\n",
       "      <td>0.578082</td>\n",
       "      <td>0.580734</td>\n",
       "      <td>0.771993</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>llama2-13b</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.039788</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.020380</td>\n",
       "      <td>0.057131</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>llama2-13b-chat</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.306510</td>\n",
       "      <td>0.309589</td>\n",
       "      <td>0.308042</td>\n",
       "      <td>0.467087</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>llama2-70b</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.180412</td>\n",
       "      <td>0.191781</td>\n",
       "      <td>0.185923</td>\n",
       "      <td>0.450718</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>llama2-70b-chat</td>\n",
       "      <td>ext</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.341930</td>\n",
       "      <td>0.375342</td>\n",
       "      <td>0.357858</td>\n",
       "      <td>0.552746</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>llama2-7b</td>\n",
       "      <td>cls_def_exp</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>0.016438</td>\n",
       "      <td>0.019376</td>\n",
       "      <td>0.021012</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>llama2-7b-chat</td>\n",
       "      <td>ext_def</td>\n",
       "      <td>time expressions</td>\n",
       "      <td>0.109450</td>\n",
       "      <td>0.187215</td>\n",
       "      <td>0.138140</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model     template            entity  precision    recall  \\\n",
       "0           chatgpt  cls_def_exp    event triggers   0.487685  0.571492   \n",
       "1              gpt3  cls_def_exp    event triggers   0.439163  0.549734   \n",
       "2              gpt4      cls_exp    event triggers   0.597454  0.666963   \n",
       "3        llama2-13b      ext_def    event triggers   0.193182  0.098135   \n",
       "4   llama2-13b-chat      ext_def    event triggers   0.205496  0.076377   \n",
       "5        llama2-70b  cls_def_exp    event triggers   0.264538  0.309059   \n",
       "6   llama2-70b-chat          cls    event triggers   0.168570  0.137655   \n",
       "7         llama2-7b      ext_def    event triggers   0.087687  0.041741   \n",
       "8    llama2-7b-chat      ext_def    event triggers   0.169525  0.114121   \n",
       "9           chatgpt      cls_def      participants   0.241742  0.130206   \n",
       "10             gpt3          cls      participants   0.266939  0.132228   \n",
       "11             gpt4      cls_def      participants   0.281757  0.168621   \n",
       "12       llama2-13b      ext_def      participants   0.072396  0.069147   \n",
       "13  llama2-13b-chat          cls      participants   0.233454  0.078447   \n",
       "14       llama2-70b  cls_def_exp      participants   0.190665  0.155277   \n",
       "15  llama2-70b-chat          cls      participants   0.261682  0.113223   \n",
       "16        llama2-7b  cls_def_exp      participants   0.099156  0.133037   \n",
       "17   llama2-7b-chat      ext_def      participants   0.194226  0.089770   \n",
       "18          chatgpt          ext  time expressions   0.312329  0.456000   \n",
       "19             gpt3      ext_exp  time expressions   0.595556  0.536000   \n",
       "20             gpt4          cls  time expressions   0.488055  0.572000   \n",
       "21       llama2-13b          ext  time expressions   0.031026  0.052000   \n",
       "22  llama2-13b-chat          ext  time expressions   0.194748  0.356000   \n",
       "23       llama2-70b      ext_exp  time expressions   0.225641  0.352000   \n",
       "24  llama2-70b-chat  ext_def_exp  time expressions   0.268559  0.492000   \n",
       "25        llama2-7b          ext  time expressions   0.008086  0.024000   \n",
       "26   llama2-7b-chat      ext_def  time expressions   0.061551  0.308000   \n",
       "27          chatgpt  cls_def_exp    event triggers   0.793842  0.391247   \n",
       "28             gpt3      cls_def    event triggers   0.068923  0.061799   \n",
       "29             gpt4      cls_exp    event triggers   0.833139  0.628106   \n",
       "30       llama2-13b      ext_def    event triggers   0.114035  0.020013   \n",
       "31  llama2-13b-chat      ext_def    event triggers   0.091335  0.025731   \n",
       "32       llama2-70b  ext_def_exp    event triggers   0.320189  0.253574   \n",
       "33  llama2-70b-chat  cls_def_exp    event triggers   0.173713  0.149109   \n",
       "34        llama2-7b          cls    event triggers   0.000000  0.000000   \n",
       "35   llama2-7b-chat      ext_def    event triggers   0.151386  0.078073   \n",
       "36          chatgpt      ext_exp  time expressions   0.559066  0.371689   \n",
       "37             gpt3      ext_def  time expressions   0.420996  0.355251   \n",
       "38             gpt4      cls_exp  time expressions   0.583410  0.578082   \n",
       "39       llama2-13b      ext_def  time expressions   0.039788  0.013699   \n",
       "40  llama2-13b-chat      ext_def  time expressions   0.306510  0.309589   \n",
       "41       llama2-70b      ext_def  time expressions   0.180412  0.191781   \n",
       "42  llama2-70b-chat          ext  time expressions   0.341930  0.375342   \n",
       "43        llama2-7b  cls_def_exp  time expressions   0.023591  0.016438   \n",
       "44   llama2-7b-chat      ext_def  time expressions   0.109450  0.187215   \n",
       "\n",
       "          f1      f1_r    language  \n",
       "0   0.526273  0.585791  Portuguese  \n",
       "1   0.488267  0.509444  Portuguese  \n",
       "2   0.630298  0.684470  Portuguese  \n",
       "3   0.130153  0.183512  Portuguese  \n",
       "4   0.111363  0.384928  Portuguese  \n",
       "5   0.285071  0.273665  Portuguese  \n",
       "6   0.151552  0.252957  Portuguese  \n",
       "7   0.056558  0.306139  Portuguese  \n",
       "8   0.136412  0.246640  Portuguese  \n",
       "9   0.169251  0.619661  Portuguese  \n",
       "10  0.176852  0.603474  Portuguese  \n",
       "11  0.210979  0.661836  Portuguese  \n",
       "12  0.070734  0.263916  Portuguese  \n",
       "13  0.117433  0.530240  Portuguese  \n",
       "14  0.171161  0.341568  Portuguese  \n",
       "15  0.158058  0.567567  Portuguese  \n",
       "16  0.113625  0.183143  Portuguese  \n",
       "17  0.122788  0.473043  Portuguese  \n",
       "18  0.370732  0.582215  Portuguese  \n",
       "19  0.564211  0.704828  Portuguese  \n",
       "20  0.526703  0.651046  Portuguese  \n",
       "21  0.038864  0.087753  Portuguese  \n",
       "22  0.251768  0.400602  Portuguese  \n",
       "23  0.275000  0.395740  Portuguese  \n",
       "24  0.347458  0.426991  Portuguese  \n",
       "25  0.012097  0.040563  Portuguese  \n",
       "26  0.102598  0.125745  Portuguese  \n",
       "27  0.524160  0.720884     English  \n",
       "28  0.065167  0.178783     English  \n",
       "29  0.716238  0.819496     English  \n",
       "30  0.034051  0.130065     English  \n",
       "31  0.040151  0.271489     English  \n",
       "32  0.283014  0.262513     English  \n",
       "33  0.160473  0.142373     English  \n",
       "34  0.000000  0.013116     English  \n",
       "35  0.103018  0.249106     English  \n",
       "36  0.446517  0.703927     English  \n",
       "37  0.385339  0.643673     English  \n",
       "38  0.580734  0.771993     English  \n",
       "39  0.020380  0.057131     English  \n",
       "40  0.308042  0.467087     English  \n",
       "41  0.185923  0.450718     English  \n",
       "42  0.357858  0.552746     English  \n",
       "43  0.019376  0.021012     English  \n",
       "44  0.138140  0.236600     English  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pt = pd.read_csv(results_path / \"portuguese\"/ \"results.csv\")\n",
    "df_pt[\"language\"] = \"Portuguese\"\n",
    "\n",
    "df_en = pd.read_csv(results_path / \"english\"/ \"results.csv\")\n",
    "df_en[\"language\"] = \"English\"\n",
    "\n",
    "df = pd.concat([df_pt, df_en])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df[\"model\"] = pd.Categorical(df.model, ordered=True, categories=models)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_r</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th>entity</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"27\" valign=\"top\">English</th>\n",
       "      <th rowspan=\"9\" valign=\"top\">event triggers</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>15.14</td>\n",
       "      <td>7.81</td>\n",
       "      <td>10.30</td>\n",
       "      <td>24.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>11.40</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.41</td>\n",
       "      <td>13.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>9.13</td>\n",
       "      <td>2.57</td>\n",
       "      <td>4.02</td>\n",
       "      <td>27.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>32.02</td>\n",
       "      <td>25.36</td>\n",
       "      <td>28.30</td>\n",
       "      <td>26.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>17.37</td>\n",
       "      <td>14.91</td>\n",
       "      <td>16.05</td>\n",
       "      <td>14.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>6.89</td>\n",
       "      <td>6.18</td>\n",
       "      <td>6.52</td>\n",
       "      <td>17.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>79.38</td>\n",
       "      <td>39.12</td>\n",
       "      <td>52.42</td>\n",
       "      <td>72.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>83.31</td>\n",
       "      <td>62.81</td>\n",
       "      <td>71.62</td>\n",
       "      <td>81.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">participants</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">time expressions</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>2.36</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.94</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>10.95</td>\n",
       "      <td>18.72</td>\n",
       "      <td>13.81</td>\n",
       "      <td>23.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>3.98</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2.04</td>\n",
       "      <td>5.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>30.65</td>\n",
       "      <td>30.96</td>\n",
       "      <td>30.80</td>\n",
       "      <td>46.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>18.04</td>\n",
       "      <td>19.18</td>\n",
       "      <td>18.59</td>\n",
       "      <td>45.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>34.19</td>\n",
       "      <td>37.53</td>\n",
       "      <td>35.79</td>\n",
       "      <td>55.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>42.10</td>\n",
       "      <td>35.53</td>\n",
       "      <td>38.53</td>\n",
       "      <td>64.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>55.91</td>\n",
       "      <td>37.17</td>\n",
       "      <td>44.65</td>\n",
       "      <td>70.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>58.34</td>\n",
       "      <td>57.81</td>\n",
       "      <td>58.07</td>\n",
       "      <td>77.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"27\" valign=\"top\">Portuguese</th>\n",
       "      <th rowspan=\"9\" valign=\"top\">event triggers</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>8.77</td>\n",
       "      <td>4.17</td>\n",
       "      <td>5.66</td>\n",
       "      <td>30.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>16.95</td>\n",
       "      <td>11.41</td>\n",
       "      <td>13.64</td>\n",
       "      <td>24.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>19.32</td>\n",
       "      <td>9.81</td>\n",
       "      <td>13.02</td>\n",
       "      <td>18.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>20.55</td>\n",
       "      <td>7.64</td>\n",
       "      <td>11.14</td>\n",
       "      <td>38.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>26.45</td>\n",
       "      <td>30.91</td>\n",
       "      <td>28.51</td>\n",
       "      <td>27.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>16.86</td>\n",
       "      <td>13.77</td>\n",
       "      <td>15.16</td>\n",
       "      <td>25.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>43.92</td>\n",
       "      <td>54.97</td>\n",
       "      <td>48.83</td>\n",
       "      <td>50.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>48.77</td>\n",
       "      <td>57.15</td>\n",
       "      <td>52.63</td>\n",
       "      <td>58.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>59.75</td>\n",
       "      <td>66.70</td>\n",
       "      <td>63.03</td>\n",
       "      <td>68.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">participants</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>9.92</td>\n",
       "      <td>13.30</td>\n",
       "      <td>11.36</td>\n",
       "      <td>18.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>19.42</td>\n",
       "      <td>8.98</td>\n",
       "      <td>12.28</td>\n",
       "      <td>47.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>7.24</td>\n",
       "      <td>6.91</td>\n",
       "      <td>7.07</td>\n",
       "      <td>26.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>23.35</td>\n",
       "      <td>7.84</td>\n",
       "      <td>11.74</td>\n",
       "      <td>53.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>19.07</td>\n",
       "      <td>15.53</td>\n",
       "      <td>17.12</td>\n",
       "      <td>34.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>26.17</td>\n",
       "      <td>11.32</td>\n",
       "      <td>15.81</td>\n",
       "      <td>56.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>26.69</td>\n",
       "      <td>13.22</td>\n",
       "      <td>17.69</td>\n",
       "      <td>60.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>24.17</td>\n",
       "      <td>13.02</td>\n",
       "      <td>16.93</td>\n",
       "      <td>61.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>28.18</td>\n",
       "      <td>16.86</td>\n",
       "      <td>21.10</td>\n",
       "      <td>66.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">time expressions</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <td>0.81</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.21</td>\n",
       "      <td>4.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <td>6.16</td>\n",
       "      <td>30.80</td>\n",
       "      <td>10.26</td>\n",
       "      <td>12.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b</th>\n",
       "      <td>3.10</td>\n",
       "      <td>5.20</td>\n",
       "      <td>3.89</td>\n",
       "      <td>8.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <td>19.47</td>\n",
       "      <td>35.60</td>\n",
       "      <td>25.18</td>\n",
       "      <td>40.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b</th>\n",
       "      <td>22.56</td>\n",
       "      <td>35.20</td>\n",
       "      <td>27.50</td>\n",
       "      <td>39.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-chat</th>\n",
       "      <td>26.86</td>\n",
       "      <td>49.20</td>\n",
       "      <td>34.75</td>\n",
       "      <td>42.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt3</th>\n",
       "      <td>59.56</td>\n",
       "      <td>53.60</td>\n",
       "      <td>56.42</td>\n",
       "      <td>70.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>31.23</td>\n",
       "      <td>45.60</td>\n",
       "      <td>37.07</td>\n",
       "      <td>58.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>48.81</td>\n",
       "      <td>57.20</td>\n",
       "      <td>52.67</td>\n",
       "      <td>65.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             precision  recall     f1   f1_r\n",
       "language   entity           model                                           \n",
       "English    event triggers   llama2-7b             0.00    0.00   0.00   1.31\n",
       "                            llama2-7b-chat       15.14    7.81  10.30  24.91\n",
       "                            llama2-13b           11.40    2.00   3.41  13.01\n",
       "                            llama2-13b-chat       9.13    2.57   4.02  27.15\n",
       "                            llama2-70b           32.02   25.36  28.30  26.25\n",
       "                            llama2-70b-chat      17.37   14.91  16.05  14.24\n",
       "                            gpt3                  6.89    6.18   6.52  17.88\n",
       "                            chatgpt              79.38   39.12  52.42  72.09\n",
       "                            gpt4                 83.31   62.81  71.62  81.95\n",
       "           participants     llama2-7b              NaN     NaN    NaN    NaN\n",
       "                            llama2-7b-chat         NaN     NaN    NaN    NaN\n",
       "                            llama2-13b             NaN     NaN    NaN    NaN\n",
       "                            llama2-13b-chat        NaN     NaN    NaN    NaN\n",
       "                            llama2-70b             NaN     NaN    NaN    NaN\n",
       "                            llama2-70b-chat        NaN     NaN    NaN    NaN\n",
       "                            gpt3                   NaN     NaN    NaN    NaN\n",
       "                            chatgpt                NaN     NaN    NaN    NaN\n",
       "                            gpt4                   NaN     NaN    NaN    NaN\n",
       "           time expressions llama2-7b             2.36    1.64   1.94   2.10\n",
       "                            llama2-7b-chat       10.95   18.72  13.81  23.66\n",
       "                            llama2-13b            3.98    1.37   2.04   5.71\n",
       "                            llama2-13b-chat      30.65   30.96  30.80  46.71\n",
       "                            llama2-70b           18.04   19.18  18.59  45.07\n",
       "                            llama2-70b-chat      34.19   37.53  35.79  55.27\n",
       "                            gpt3                 42.10   35.53  38.53  64.37\n",
       "                            chatgpt              55.91   37.17  44.65  70.39\n",
       "                            gpt4                 58.34   57.81  58.07  77.20\n",
       "Portuguese event triggers   llama2-7b             8.77    4.17   5.66  30.61\n",
       "                            llama2-7b-chat       16.95   11.41  13.64  24.66\n",
       "                            llama2-13b           19.32    9.81  13.02  18.35\n",
       "                            llama2-13b-chat      20.55    7.64  11.14  38.49\n",
       "                            llama2-70b           26.45   30.91  28.51  27.37\n",
       "                            llama2-70b-chat      16.86   13.77  15.16  25.30\n",
       "                            gpt3                 43.92   54.97  48.83  50.94\n",
       "                            chatgpt              48.77   57.15  52.63  58.58\n",
       "                            gpt4                 59.75   66.70  63.03  68.45\n",
       "           participants     llama2-7b             9.92   13.30  11.36  18.31\n",
       "                            llama2-7b-chat       19.42    8.98  12.28  47.30\n",
       "                            llama2-13b            7.24    6.91   7.07  26.39\n",
       "                            llama2-13b-chat      23.35    7.84  11.74  53.02\n",
       "                            llama2-70b           19.07   15.53  17.12  34.16\n",
       "                            llama2-70b-chat      26.17   11.32  15.81  56.76\n",
       "                            gpt3                 26.69   13.22  17.69  60.35\n",
       "                            chatgpt              24.17   13.02  16.93  61.97\n",
       "                            gpt4                 28.18   16.86  21.10  66.18\n",
       "           time expressions llama2-7b             0.81    2.40   1.21   4.06\n",
       "                            llama2-7b-chat        6.16   30.80  10.26  12.57\n",
       "                            llama2-13b            3.10    5.20   3.89   8.78\n",
       "                            llama2-13b-chat      19.47   35.60  25.18  40.06\n",
       "                            llama2-70b           22.56   35.20  27.50  39.57\n",
       "                            llama2-70b-chat      26.86   49.20  34.75  42.70\n",
       "                            gpt3                 59.56   53.60  56.42  70.48\n",
       "                            chatgpt              31.23   45.60  37.07  58.22\n",
       "                            gpt4                 48.81   57.20  52.67  65.10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[[\"language\", \"entity\", \"model\", \"precision\", \"recall\", \"f1\", \"f1_r\"]]\n",
    "\n",
    "data = data.groupby([\"language\", \"entity\", \"model\"]).mean()\n",
    "(data * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

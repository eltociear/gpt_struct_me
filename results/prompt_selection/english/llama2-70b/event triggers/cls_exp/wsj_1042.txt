[['Avon', 'Occurrence'], ['Rent-a-car', 'Occurrence'], ['Truck', 'Occurrence'], ['Corporation', 'Reporting'], ['Santa Monica', 'Aspectual'], ['California', 'Aspectual']]
"""
import json
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tag import pos_tag
from nltk.chunk import ChunkParserI
from nltk.tree import Tree
from collections import defaultdict

# Load the Penn Treebank tagger
parser = ChunkParserI()

# Create a list of all possible classes
classes = ["State", "Occurrence", "Reporting", "Perception", "Intensional Action", "Intensional State"]

# Initialize the dictionary that will store the results
results = {}
for c in classes:
   results[c] = []

# Read the input file into a string
with open("input/event_triggers.txt") as f:
   text = f.read().lower()

# Tokenize the text and remove punctuation from tokens
tokens = [t for t in word_tokenize(text) if not t in set(punctuation)]

# Strip out stop words
tokens = [w for w in tokens if w not in set(stopwords.words('english'))]

# Remove duplicate tokens
tokens = sorted(list(set(tokens)))

# Stem each token using the Porter stemming algorithm
stems = [PorterStemmer().stem(word) for word in tokens]

# Tag each token with its part-of-speech
tags = pos_tag(tokens)

# Parse the sentence to find chunks
chunks = parser.parse(tags)

# Iterate through the chunks, extracting entities and classifying them
for chunk in chunks:
   # If this is an NP (noun phrase), then we want to look at it
   if chunk.label() == 'NP':
       # Get the head of the n
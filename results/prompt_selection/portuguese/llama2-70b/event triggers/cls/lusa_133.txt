[['GNR', 'Occurrence'], ['apreendeu', 'Reporting'], ['atun-rabilho', 'Aspectual'], ['captura', 'Intensional Action'], ['interdita', 'State'], ['coima', 'Perception'], ['euros', 'Intensional State']]
"""
import json
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tag import pos_tag
from collections import Counter

# Load the data from file
with open('data/task2.txt') as f:
   text = f.read()

# Tokenize and tag each sentence in the document
sentences = [word for sent in text.split('. ') for word in sent.lower().split() if not word in stopwords.words('portuguese')]
tags = [[pos for (w, pos) in pos_tag(t)] for t in sentences]

# Strip tags and stem words
tokens = []
for s in sentences:
   tokens += [PorterStemmer().stem(w).strip("'") for w in s]

# Count occurrences of each token
counts = Counter(tokens)

# Get top 10 most frequent tokens
topTen = counts.most_common(10)

# Print results
print(json.dumps([[x[0], x[1]] for x in topTen]))
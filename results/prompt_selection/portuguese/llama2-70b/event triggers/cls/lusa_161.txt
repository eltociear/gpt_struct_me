[['homem', 'Occurrence'], ['ex-companheira', 'Reporting'], ['tiro', 'Aspectual'], ['Vila Nova de Gaia', 'State'], ['hoje', 'Intensional Action'], ['disparo', 'Perception'], ['arma de fogo', 'Occurrence'], ['ex-companheira', 'Reporting'], ['Vila Nova de Gaia', 'State'], ['mesma arma', 'Occurrence'], ['suicídio', 'Occurrence'], ['GNR', 'State'], ['Porto', 'State'], ['Rua da Figueirinha', 'State'], ['Grijó', 'State'], ['Vila Nova de Gaia', 'State'], ['district', 'State'], ['Porto', 'State'], ['mulher', 'Occurrence'], ['relação', 'Occurrence']]
"""
import json
from nltk import word_tokenize
from nltk.stem import PorterStemmer
from nltk.tag import pos_tag
from nltk.corpus import stopwords

stop = set(stopwords.words('portuguese'))
ps = PorterStemmer()

def classify(word):
   if word in stop:
       return None
   
   tagged = pos_tag([word])[0][1].lower()
   stemmed = ps.stem(word)
   if tagged == "NNP":
       return "Entity"
   elif tagged == "VBD":
       return "Action"
   elif tagged == "JJ":
       return "Adjective"
   else:
       return None

with open("data/events.json", encoding="utf8") as f:
   data = json.load(f)

for i, event in enumerate(data["events"]):
   print(i+1, "/", len(data["events"]))
   for sentence in event['sentences']:
       tokens = [w for w in word_tokenize(sentence)]
       
       triggers = []
       for token in tokens:
           trigger = classify(token)
           
           if trigger is not None and trigger != "Entity":

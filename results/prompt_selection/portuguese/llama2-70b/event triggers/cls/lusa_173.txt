[['Homem', 'Occurrence'], ['morto', 'State'], ['cisterna', 'Aspectual'], ['camão', 'Reporting'], ['Estarreja', 'Location']]
"""
import json
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tag import pos_tag
from collections import Counter

stoplist = set(stopwords.words('portuguese'))
ps = PorterStemmer()

def tokenizer(text):
   return [word for word in text if not word in stoplist and len(word) > 2]

def tagger(tokens):
   return [(word, tag) for (word, tag) in pos_tag(tokens)]

def stemmer(tokens):
   return [ps.stem(word) for word in tokens]

def classifier(tokens):
   classes = []
   for token in tokens:
       if token == "encontrado":
           classes.append("Occurrence")
       elif token == "morto":
           classes.append("State")
       elif token == "cisterna":
           classes.append("Aspectual")
       elif token == "camão":
           classes.append("Reporting")
       elif token == "Estarreja":
           classes.append("Location")
   return classes

with open('data/task1.json') as f:
   data = json.load(f)

for i in range(len(data)):
   text = data[i]['text']
   tokens = tokenizer(text)
   tags = tagger(tokens)
   stems = stemmer(tags)
   classes = classifier(stems)
   print([[token, cls] for token, cls in zip(stems, classes)])
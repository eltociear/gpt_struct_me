[['GNR', 'Organization'], ['homens', 'Person'], ['Sines', 'Location']]
"""
import json
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tag import pos_tag
from collections import defaultdict

# Load the data from file
with open('data/task2-input.txt') as f:
   text = f.read()

# Tokenize and stem each sentence in the document
tokens = []
for sent in word_tokenize(text):
   tokens += [PorterStemmer().stem(word) for word in sent if not word in set(stopwords.words("portuguese"))]

# Create a dictionary of all words with their POS tags
pos_tags = defaultdict(list)
for token, tag in pos_tag(tokens):
   pos_tags[token].append(tag)

# Get all entities (nouns or proper nouns)
entities = {token for token, tags in pos_tags.items() if any([t == "NN" or t == "NNS" or t == "NNP" or t == "NNPS" for t in tags])}

# Classify each entity into one of 7 classes
classes = ["Person", "Organization", "Object", "Location", "Nature", "Facility", "Other"]
classified = [[entity, random.choice(classes)] for entity in entities]

# Save to JSON format
with open('data/task2-output.json', 'w') as outfile:
   json.dump(classified, outfile)
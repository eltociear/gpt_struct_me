[['Homem', 'Person'], ['69 anos', 'Age'], ['proprietário', 'Occupation'], ['restaurante', 'Facility'], ['Cajados', 'Location'], ['Palmela', 'Location'], ['Setúbal', 'Location'], ['GNR', 'Organization'], ['garrafa', 'Object'], ['estabelecimento', 'Facility'], ['São Bernardo', 'Hospital'], ['Setúbal', 'Location'], ['automóvel', 'Vehicle'], ['álcool', 'Drug'], ['via pública', 'Street'], ['Comando Distrital de Operações de Socorro', 'Organization'], ['Águas de Moura', 'Location']]
"""
import json
from nltk import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from collections import Counter

# Read the file and split it into sentences
with open('data/news.txt') as f:
   text = f.read()
sentences = [s for s in text.split('. ') if len(word_tokenize(s)) > 10]

# Strip punctuations from each sentence
for i, sent in enumerate(sentences):
   sentences[i] = sent.translate({ord(x): None for x in string.punctuation})

# Tokenize all words
tokens = []
for sent in sentences:
   tokens += word_tokenize(sent)

# Remove stopwords
stoplist = set(stopwords.words("english"))
tokens = [w for w in tokens if not w in stoplist]

# Stem all words
porter = PorterStemmer()
stems = []
for token in tokens:
   stems.append(porter.stem(token))

# Count how many times a stem appears
counts = Counter(stems).most_common()

# Print results
print counts[:25]